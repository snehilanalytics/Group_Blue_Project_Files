{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=pd.read_csv('modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['description']=c['description'].replace('unknowntoken','unknown token')\n",
    "c['details']=c['details'].replace('unknowntoken','unknown token')\n",
    "c['brand_category']=c['brand_category'].replace('unknowntoken','unknown token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>details</th>\n",
       "      <th>name</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>is_work</th>\n",
       "      <th>is_nightout</th>\n",
       "      <th>is_daytonight</th>\n",
       "      <th>is_vacation</th>\n",
       "      <th>is_workout</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_coldweather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, index, product_id, description, details, name, brand_category, is_work, is_nightout, is_daytonight, is_vacation, is_workout, is_weekend, is_coldweather]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_occasion(brand,description,brand_category,wv):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    modeling=pd.read_csv('modeling.csv')\n",
    "    modeling['description']=modeling['description'].replace('unknowntoken','unknown token')\n",
    "    modeling['details']=modeling['details'].replace('unknowntoken','unknown token')\n",
    "    modeling['brand_category']=modeling['brand_category'].replace('unknowntoken','unknown token')\n",
    "    #Clean the query \n",
    "    \n",
    "    def replace(word):\n",
    "        word=word.replace('/',' ')\n",
    "        new=re.sub('[^a-zA-Z]', ' ',word)\n",
    "        p=[]\n",
    "        for k in new.split():\n",
    "            separated=re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ',k)\n",
    "            new = separated.lower()\n",
    "            p.append(new)\n",
    "        ls=WordNetLemmatizer()\n",
    "        new = [ls.lemmatize(word) for word in p if not word in set(stopwords.words('english'))]\n",
    "        new = ' '.join(new)\n",
    "        return new\n",
    "    \n",
    "    brand=replace(brand)\n",
    "    description=replace(description)\n",
    "    brand_category=replace(brand_category)\n",
    "    \n",
    "    \n",
    "    def word_averaging(wv, words):\n",
    "        all_words, mean = set(), []\n",
    "\n",
    "        for word in words:\n",
    "            if isinstance(word, np.ndarray):\n",
    "                mean.append(word)\n",
    "            elif word in wv.vocab:\n",
    "                mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "                all_words.add(wv.vocab[word].index)\n",
    "\n",
    "        if not mean:\n",
    "            logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "            # FIXME: remove these examples in pre-processing\n",
    "            return np.zeros(wv.vector_size,)\n",
    "\n",
    "        mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "        return mean\n",
    "\n",
    "    def  word_averaging_list(wv, text_list):\n",
    "        return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "\n",
    "    def w2v_tokenize_text(text):\n",
    "        tokens = []\n",
    "        for sent in nltk.sent_tokenize(text, language='english'):\n",
    "            for word in nltk.word_tokenize(sent, language='english'):\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                tokens.append(word)\n",
    "        return tokens\n",
    "    \n",
    "    c=pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    brand=w2v_tokenize_text(brand)\n",
    "    description=w2v_tokenize_text(description)\n",
    "    brand_category=w2v_tokenize_text(brand_category)\n",
    "    \n",
    "    brand=word_averaging(wv,brand).reshape(1,-1)\n",
    "    description=word_averaging(wv,description).reshape(1,-1)\n",
    "    brand_category=word_averaging(wv,brand_category).reshape(1,-1)\n",
    "    X4=np.concatenate((brand,description,brand_category),axis=1)\n",
    "    \n",
    "    \n",
    "    names=['description','details','brand_category']\n",
    "    for i in names:\n",
    "        X_new = modeling.apply(lambda r: w2v_tokenize_text(r[i]),axis=1).values\n",
    "        X_word_average = word_averaging_list(wv,X_new)\n",
    "        X_array=pd.DataFrame(X_word_average)\n",
    "        c=pd.concat([c,X_array],axis=1)\n",
    "    tags=['is_work', 'is_nightout', 'is_daytonight', 'is_vacation', 'is_workout',\n",
    "       'is_weekend', 'is_coldweather']\n",
    "    for i in tags:\n",
    "        y=modeling[i].values\n",
    "        SVM = svm.SVC(C=0.1, kernel='linear', degree=3, gamma='auto')\n",
    "        SVM.fit(c,y)\n",
    "        # predict the labels on validation dataset\n",
    "        result= SVM.predict(X4)\n",
    "        print(result,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc=\"blush linen button fastenings along front 100% linen, lining: 100%, cotton dry clean designer color: shell imported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand='Zimmermann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_cat='clothing/jumpsuit/full length'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute similarity with no input ['zimmermann']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.] is_work\n",
      "[0.] is_nightout\n",
      "[1.] is_daytonight\n",
      "[0.] is_vacation\n",
      "[0.] is_workout\n",
      "[1.] is_weekend\n",
      "[0.] is_coldweather\n"
     ]
    }
   ],
   "source": [
    "predict_occasion(desc,brand,brand_cat,wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=input(\"Please enter the brand\")\n",
    "description=input(\"Please enter description\")\n",
    "brand_cat=input(\"Please enter brand category\")\n",
    "c=predict_cat(brand,description,brand_cat,wv)\n",
    "print(\"The prediction is \",c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
