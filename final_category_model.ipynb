{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the cleaned data set with attribute name-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=pd.read_csv('c.csv')\n",
    "y=c['attribute_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat=c[['product_id','description','details','name','brand_category']].reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=c['attribute_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cat(brand,description,brand_category,wv):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    c=pd.read_csv('c.csv')\n",
    "    df_cat=c[['product_id','description','details','name','brand_category']].reset_index().drop(columns='index')\n",
    "    y=c['attribute_value']\n",
    "    #Clean the query\n",
    "    def replace(word):\n",
    "        word=word.replace('/',' ')\n",
    "        new=re.sub('[^a-zA-Z]', ' ',word)\n",
    "        p=[]\n",
    "        for k in new.split():\n",
    "            separated=re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ',k)\n",
    "            new = separated.lower()\n",
    "            p.append(new)\n",
    "        ls=WordNetLemmatizer()\n",
    "        new = [ls.lemmatize(word) for word in p if not word in set(stopwords.words('english'))]\n",
    "        new = ' '.join(new)\n",
    "        return new\n",
    "    brand=replace(brand)\n",
    "    description=replace(description)\n",
    "    brand_category=replace(brand_category)\n",
    "    import gensim\n",
    "    from gensim.models import Word2Vec\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "    wv.init_sims(replace=True)\n",
    "    def word_averaging(wv, words):\n",
    "        all_words, mean = set(), []\n",
    "\n",
    "        for word in words:\n",
    "            if isinstance(word, np.ndarray):\n",
    "                mean.append(word)\n",
    "            elif word in wv.vocab:\n",
    "                mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "                all_words.add(wv.vocab[word].index)\n",
    "\n",
    "        if not mean:\n",
    "            logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "            # FIXME: remove these examples in pre-processing\n",
    "            return np.zeros(wv.vector_size,)\n",
    "\n",
    "        mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "        return mean\n",
    "\n",
    "    def  word_averaging_list(wv, text_list):\n",
    "        return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "    def w2v_tokenize_text(text):\n",
    "        tokens = []\n",
    "        for sent in nltk.sent_tokenize(text, language='english'):\n",
    "            for word in nltk.word_tokenize(sent, language='english'):\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                tokens.append(word)\n",
    "        return tokens\n",
    "    names=['description','brand_category','details']\n",
    "    d=pd.DataFrame()\n",
    "    \n",
    "    brand=w2v_tokenize_text(brand)\n",
    "    description=w2v_tokenize_text(description)\n",
    "    brand_category=w2v_tokenize_text(brand_category)\n",
    "    \n",
    "    brand=word_averaging(wv,brand).reshape(1,-1)\n",
    "    description=word_averaging(wv,description).reshape(1,-1)\n",
    "    brand_category=word_averaging(wv,brand_category).reshape(1,-1)\n",
    "    X4=np.concatenate((brand,description,brand_category),axis=1)\n",
    "\n",
    "    for i in names:\n",
    "        X_new = df_cat.apply(lambda r: w2v_tokenize_text(r[i]),axis=1).values\n",
    "        X_word_average = word_averaging_list(wv,X_new)\n",
    "        X_array=pd.DataFrame(X_word_average)\n",
    "        d=pd.concat([d,X_array],axis=1)\n",
    "\n",
    "    nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,50)).fit(d, y)\n",
    "    return nn.predict(X4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=input(\"Please enter the brand\")\n",
    "description=input(\"Please enter description\")\n",
    "brand_cat=input(\"Please enter brand category\")\n",
    "c=predict_cat(brand,description,brand_cat,wv)\n",
    "print(\"The prediction is \",c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
