{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Full+data.csv')\n",
    "extra=pd.read_csv('extra_data.csv')\n",
    "attributes=pd.read_excel('Womens+Attributes.xlsx')\n",
    "USC_product_attributes=pd.read_excel('USC+Product+Attribute+Data+03302020.xlsx')\n",
    "attributes=attributes[['Style',\n",
    "       '(Reference Style Lookbook, choose all that apply)']]\n",
    "tags=pd.read_csv('usc_additional_tags.csv')\n",
    "#Dropped all columns with ALL null values\n",
    "data_dropped=data.dropna(axis=1,how='all')\n",
    "#Attach additional data to full data\n",
    "full_data=pd.concat([data_dropped,extra])\n",
    "full_data.drop_duplicates(subset=['product_id'], keep=\"first\",inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and lemmatizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=full_data[['product_id','description','details','name','brand_category']]\n",
    "full_data.fillna('UNKNOWN_TOKEN',inplace=True)\n",
    "subset_for_model=full_data\n",
    "subset_for_model['brand_category']=subset_for_model['brand_category'].str.replace('/',' ')\n",
    "subset_for_model['brand_category']=subset_for_model['brand_category'].str.replace(',',' ')\n",
    "subset_for_model['details']=subset_for_model['brand_category'].str.replace('\\n',' ')\n",
    "subset_for_model['details']=subset_for_model['brand_category'].str.replace('/',' ')\n",
    "subset_for_model['details']=subset_for_model['brand_category'].str.replace(',',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_model=subset_for_model.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shashanktiwari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "columns=['details','description','name','brand_category']\n",
    "for j in columns:\n",
    "    corpus = []\n",
    "    for i in range(0,len(subset_for_model)):\n",
    "        new=re.sub('[^a-zA-Z]', ' ',str(subset_for_model.loc[i,j]))\n",
    "        p=[]\n",
    "        for k in new.split():\n",
    "            separated=re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ',k)\n",
    "            new = separated.lower()\n",
    "            p.append(new)\n",
    "        ls=WordNetLemmatizer()\n",
    "        new = [ls.lemmatize(word) for word in p if not word in set(stopwords.words('english'))]\n",
    "        new = ' '.join(new)\n",
    "        subset_for_model.loc[i,j]=new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_model['details']=subset_for_model['details'].str.replace('dressesandjumpsuits','dresses and jumpsuits')\n",
    "subset_for_model['brand_category']=subset_for_model['brand_category'].str.replace('dressesandjumpsuits','dresses and jumpsuits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USC_product_attributes=pd.read_excel('USC+Product+Attribute+Data+03302020.xlsx')\n",
    "new_tags=pd.concat([tags,USC_product_attributes])\n",
    "new_tags=new_tags[['product_id','attribute_name','attribute_value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the cleaned datasets with the attribute table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=subset_for_model.merge(new_tags,on='product_id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting only the attributes which are for categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=c[c['attribute_name']=='category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the column 'attribute_value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['attribute_value']=c['attribute_value'].str.replace('&','')\n",
    "c['attribute_value']=c['attribute_value'].str.lower()\n",
    "c['attribute_value']=c['attribute_value'].str.replace(' ','')\n",
    "c['attribute_value']=c['attribute_value'].str.replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['top', 'onepiece', 'bottom', 'shoe', 'sweater', 'accessory',\n",
       "       'blazerscoatsjackets', 'sweatshirthoodie'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c['attribute_value'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the model into input (df_cat) and output (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=c['attribute_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat=c[['description','details','name','brand_category']].reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=pd.read_csv('c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>details</th>\n",
       "      <th>name</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>01DTJCERF6F4NRZ2WSJFFA1EYS</td>\n",
       "      <td>beige stretch silk slip silk spandex dry clean...</td>\n",
       "      <td>clothing top tank camis</td>\n",
       "      <td>teah stretch silk camisole</td>\n",
       "      <td>clothing top tank camis</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>01DTJCERF6F4NRZ2WSJFFA1EYS</td>\n",
       "      <td>beige stretch silk slip silk spandex dry clean...</td>\n",
       "      <td>clothing top tank camis</td>\n",
       "      <td>teah stretch silk camisole</td>\n",
       "      <td>clothing top tank camis</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>8</td>\n",
       "      <td>01DVPBJ6464YKYGVAE0A1HMKGN</td>\n",
       "      <td>black velvet concealed hook zip fastening back...</td>\n",
       "      <td>clothing dress mini</td>\n",
       "      <td>layered velvet mini dress</td>\n",
       "      <td>clothing dress mini</td>\n",
       "      <td>category</td>\n",
       "      <td>onepiece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>8</td>\n",
       "      <td>01DVPBJ6464YKYGVAE0A1HMKGN</td>\n",
       "      <td>black velvet concealed hook zip fastening back...</td>\n",
       "      <td>clothing dress mini</td>\n",
       "      <td>layered velvet mini dress</td>\n",
       "      <td>clothing dress mini</td>\n",
       "      <td>category</td>\n",
       "      <td>onepiece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>10</td>\n",
       "      <td>01DPETNS8DB4MAR29ER6ESA7XA</td>\n",
       "      <td>accordion pleat flattering midi length faux le...</td>\n",
       "      <td>skirt</td>\n",
       "      <td>faux leather pleated midi skirt</td>\n",
       "      <td>skirt</td>\n",
       "      <td>category</td>\n",
       "      <td>bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6775</th>\n",
       "      <td>163389</td>\n",
       "      <td>6616</td>\n",
       "      <td>01E2KZ045BCT1Q2Z2PBC0CWRGH</td>\n",
       "      <td>casual trouser cut easy drapey fit thigh taper...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>micro twill pull pant</td>\n",
       "      <td>unknown</td>\n",
       "      <td>category</td>\n",
       "      <td>bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6776</th>\n",
       "      <td>163405</td>\n",
       "      <td>6617</td>\n",
       "      <td>01E2KXZ0WNQ96S64TDZE3WB7CX</td>\n",
       "      <td>brushed twill give cropped wide leg pant subtl...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>brushed twill crop wide leg pant</td>\n",
       "      <td>unknown</td>\n",
       "      <td>category</td>\n",
       "      <td>bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6777</th>\n",
       "      <td>163417</td>\n",
       "      <td>6618</td>\n",
       "      <td>01E2KXWWY6JAGENED7P62090NP</td>\n",
       "      <td>tailored soft dense knit cropped cuffed hem le...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>slim crop pant</td>\n",
       "      <td>unknown</td>\n",
       "      <td>category</td>\n",
       "      <td>bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6778</th>\n",
       "      <td>163436</td>\n",
       "      <td>6619</td>\n",
       "      <td>01E2KZTEPE7QK1V8THASF7HWNR</td>\n",
       "      <td>flatlock stitched bias seam enhance fluid drap...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>camo print silk skirt</td>\n",
       "      <td>unknown</td>\n",
       "      <td>category</td>\n",
       "      <td>bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6779</th>\n",
       "      <td>163451</td>\n",
       "      <td>6620</td>\n",
       "      <td>01E2KY2T6YV7R4VXT4BVV4TWT5</td>\n",
       "      <td>made atm signature peruvian jersey fitted vers...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>stretch pima cotton baby tee</td>\n",
       "      <td>unknown</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6780 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  index                  product_id  \\\n",
       "0             25      6  01DTJCERF6F4NRZ2WSJFFA1EYS   \n",
       "1             45      6  01DTJCERF6F4NRZ2WSJFFA1EYS   \n",
       "2             69      8  01DVPBJ6464YKYGVAE0A1HMKGN   \n",
       "3             76      8  01DVPBJ6464YKYGVAE0A1HMKGN   \n",
       "4            107     10  01DPETNS8DB4MAR29ER6ESA7XA   \n",
       "...          ...    ...                         ...   \n",
       "6775      163389   6616  01E2KZ045BCT1Q2Z2PBC0CWRGH   \n",
       "6776      163405   6617  01E2KXZ0WNQ96S64TDZE3WB7CX   \n",
       "6777      163417   6618  01E2KXWWY6JAGENED7P62090NP   \n",
       "6778      163436   6619  01E2KZTEPE7QK1V8THASF7HWNR   \n",
       "6779      163451   6620  01E2KY2T6YV7R4VXT4BVV4TWT5   \n",
       "\n",
       "                                            description  \\\n",
       "0     beige stretch silk slip silk spandex dry clean...   \n",
       "1     beige stretch silk slip silk spandex dry clean...   \n",
       "2     black velvet concealed hook zip fastening back...   \n",
       "3     black velvet concealed hook zip fastening back...   \n",
       "4     accordion pleat flattering midi length faux le...   \n",
       "...                                                 ...   \n",
       "6775  casual trouser cut easy drapey fit thigh taper...   \n",
       "6776  brushed twill give cropped wide leg pant subtl...   \n",
       "6777  tailored soft dense knit cropped cuffed hem le...   \n",
       "6778  flatlock stitched bias seam enhance fluid drap...   \n",
       "6779  made atm signature peruvian jersey fitted vers...   \n",
       "\n",
       "                      details                              name  \\\n",
       "0     clothing top tank camis        teah stretch silk camisole   \n",
       "1     clothing top tank camis        teah stretch silk camisole   \n",
       "2         clothing dress mini         layered velvet mini dress   \n",
       "3         clothing dress mini         layered velvet mini dress   \n",
       "4                       skirt   faux leather pleated midi skirt   \n",
       "...                       ...                               ...   \n",
       "6775                  unknown             micro twill pull pant   \n",
       "6776                  unknown  brushed twill crop wide leg pant   \n",
       "6777                  unknown                    slim crop pant   \n",
       "6778                  unknown             camo print silk skirt   \n",
       "6779                  unknown      stretch pima cotton baby tee   \n",
       "\n",
       "               brand_category attribute_name attribute_value  \n",
       "0     clothing top tank camis       category             top  \n",
       "1     clothing top tank camis       category             top  \n",
       "2         clothing dress mini       category        onepiece  \n",
       "3         clothing dress mini       category        onepiece  \n",
       "4                       skirt       category          bottom  \n",
       "...                       ...            ...             ...  \n",
       "6775                  unknown       category          bottom  \n",
       "6776                  unknown       category          bottom  \n",
       "6777                  unknown       category          bottom  \n",
       "6778                  unknown       category          bottom  \n",
       "6779                  unknown       category             top  \n",
       "\n",
       "[6780 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "108\n",
      "1622\n",
      "393\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "vectorizer = TfidfVectorizer(min_df=10)\n",
    "columns=['brand_category','details','description','name']\n",
    "e=pd.DataFrame()\n",
    "for i in columns:\n",
    "    corpus = list(df_cat[i].values)\n",
    "    p = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    print(len(terms))\n",
    "    c=pd.DataFrame(p.toarray().transpose(), index=terms)\n",
    "    e=pd.concat([e,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input X for models from TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=e.T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6780, 2231)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7640117994100295\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "logreg=LogisticRegression(n_jobs=1, C=0.01)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the proportion of different categories to see how well the model performs compared to random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top                    77.522124\n",
       "shoe                   78.466077\n",
       "bottom                 79.867257\n",
       "accessory              90.560472\n",
       "onepiece               91.076696\n",
       "sweater                91.961652\n",
       "blazerscoatsjackets    93.613569\n",
       "sweatshirthoodie       96.932153\n",
       "Name: attribute_value, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-y.value_counts()/len(y))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9786135693215339 (128, 64)\n",
      "0.9778761061946902 (100, 50)\n",
      "0.9800884955752213 (50, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "size=[(128,64),(100,50),(50,20)]\n",
    "for j in size:\n",
    "    nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=j).fit(X_train, y_train)\n",
    "    y_pred_RF = nn.predict(X_test)\n",
    "    #print(\"Actual % of the data for tag\",p,\"is\",sum(subset_for_model['attribute_value'])/len(subset_for_model)*100)\n",
    "    print(accuracy_score(y_test,y_pred_RF),j)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9188790560471977"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_NB = classifier.predict(X_test)\n",
    "accuracy_score(y_test,y_pred_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9756637168141593"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "SVM = svm.SVC(C=0.1, kernel='linear', degree=3, gamma='auto')\n",
    "classifier=SVM.fit(X_train,y_train)\n",
    "y_pred_NB = classifier.predict(X_test)\n",
    "accuracy_score(y_test,y_pred_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memorial_Hospital',\n",
       " 'Seniors',\n",
       " 'memorandum',\n",
       " 'elephant',\n",
       " 'Trump',\n",
       " 'Census',\n",
       " 'pilgrims',\n",
       " 'De',\n",
       " 'Dogs',\n",
       " '###-####_ext',\n",
       " 'chaotic',\n",
       " 'forgive',\n",
       " 'scholar',\n",
       " 'Lottery',\n",
       " 'decreasing',\n",
       " 'Supervisor',\n",
       " 'fundamentally',\n",
       " 'Fitness',\n",
       " 'abundance',\n",
       " 'Hold']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['ellise']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['ellise']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['charli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['elli']\n",
      "WARNING:root:cannot compute similarity with no input ['everly']\n",
      "WARNING:root:cannot compute similarity with no input ['everly']\n",
      "WARNING:root:cannot compute similarity with no input ['arabella']\n",
      "WARNING:root:cannot compute similarity with no input ['athena']\n",
      "WARNING:root:cannot compute similarity with no input ['ashlyn']\n",
      "WARNING:root:cannot compute similarity with no input ['brenna']\n",
      "WARNING:root:cannot compute similarity with no input ['aubry']\n",
      "WARNING:root:cannot compute similarity with no input ['audrey']\n",
      "WARNING:root:cannot compute similarity with no input ['aubrey']\n",
      "WARNING:root:cannot compute similarity with no input ['ashlyn']\n",
      "WARNING:root:cannot compute similarity with no input ['beckett']\n",
      "WARNING:root:cannot compute similarity with no input ['calder']\n",
      "WARNING:root:cannot compute similarity with no input ['beckett']\n",
      "WARNING:root:cannot compute similarity with no input ['agnes']\n",
      "WARNING:root:cannot compute similarity with no input ['amaya']\n",
      "WARNING:root:cannot compute similarity with no input ['alix']\n",
      "WARNING:root:cannot compute similarity with no input ['amelie']\n",
      "WARNING:root:cannot compute similarity with no input ['agnes']\n",
      "WARNING:root:cannot compute similarity with no input ['amara']\n",
      "WARNING:root:cannot compute similarity with no input ['amara']\n",
      "WARNING:root:cannot compute similarity with no input ['callan']\n",
      "WARNING:root:cannot compute similarity with no input ['eloise']\n",
      "WARNING:root:cannot compute similarity with no input ['josephine']\n",
      "WARNING:root:cannot compute similarity with no input ['delilah']\n",
      "WARNING:root:cannot compute similarity with no input ['eloise']\n",
      "WARNING:root:cannot compute similarity with no input ['delilah']\n",
      "WARNING:root:cannot compute similarity with no input ['cozette']\n",
      "WARNING:root:cannot compute similarity with no input ['cozette']\n",
      "WARNING:root:cannot compute similarity with no input ['davie']\n",
      "WARNING:root:cannot compute similarity with no input ['davie']\n",
      "WARNING:root:cannot compute similarity with no input ['davie']\n",
      "WARNING:root:cannot compute similarity with no input ['hallie']\n",
      "WARNING:root:cannot compute similarity with no input ['valentina']\n",
      "WARNING:root:cannot compute similarity with no input ['yolie']\n",
      "WARNING:root:cannot compute similarity with no input ['febe']\n",
      "WARNING:root:cannot compute similarity with no input ['baylee']\n",
      "WARNING:root:cannot compute similarity with no input ['rhett']\n",
      "WARNING:root:cannot compute similarity with no input ['corsica']\n",
      "WARNING:root:cannot compute similarity with no input ['saylor']\n",
      "WARNING:root:cannot compute similarity with no input ['asun']\n",
      "WARNING:root:cannot compute similarity with no input ['cecilia']\n",
      "WARNING:root:cannot compute similarity with no input ['manon']\n",
      "WARNING:root:cannot compute similarity with no input ['narita']\n",
      "WARNING:root:cannot compute similarity with no input ['arlo']\n",
      "WARNING:root:cannot compute similarity with no input ['giada']\n",
      "WARNING:root:cannot compute similarity with no input ['coba']\n",
      "WARNING:root:cannot compute similarity with no input ['tatum']\n",
      "WARNING:root:cannot compute similarity with no input ['noelle']\n",
      "WARNING:root:cannot compute similarity with no input ['hierro']\n",
      "WARNING:root:cannot compute similarity with no input ['freya']\n",
      "WARNING:root:cannot compute similarity with no input ['manon']\n",
      "WARNING:root:cannot compute similarity with no input ['cancun']\n",
      "WARNING:root:cannot compute similarity with no input ['zarina']\n",
      "WARNING:root:cannot compute similarity with no input ['rhett']\n",
      "WARNING:root:cannot compute similarity with no input ['otis']\n",
      "WARNING:root:cannot compute similarity with no input ['menphi']\n",
      "WARNING:root:cannot compute similarity with no input ['rhett']\n",
      "WARNING:root:cannot compute similarity with no input ['tula']\n",
      "WARNING:root:cannot compute similarity with no input ['keiko']\n",
      "WARNING:root:cannot compute similarity with no input ['kyra']\n",
      "WARNING:root:cannot compute similarity with no input ['chimo']\n",
      "WARNING:root:cannot compute similarity with no input ['moscot']\n",
      "WARNING:root:cannot compute similarity with no input ['conny']\n",
      "WARNING:root:cannot compute similarity with no input ['selah']\n",
      "WARNING:root:cannot compute similarity with no input ['tatum']\n",
      "WARNING:root:cannot compute similarity with no input ['narita']\n",
      "WARNING:root:cannot compute similarity with no input ['samara']\n",
      "WARNING:root:cannot compute similarity with no input ['chimo']\n",
      "WARNING:root:cannot compute similarity with no input ['arlo']\n",
      "WARNING:root:cannot compute similarity with no input ['yolie']\n",
      "WARNING:root:cannot compute similarity with no input ['messico']\n",
      "WARNING:root:cannot compute similarity with no input ['moscot']\n",
      "WARNING:root:cannot compute similarity with no input ['leba']\n",
      "WARNING:root:cannot compute similarity with no input ['manon']\n",
      "WARNING:root:cannot compute similarity with no input ['sonnie']\n",
      "WARNING:root:cannot compute similarity with no input ['alayna']\n",
      "WARNING:root:cannot compute similarity with no input ['chimo']\n",
      "WARNING:root:cannot compute similarity with no input ['zefir']\n",
      "WARNING:root:cannot compute similarity with no input ['saylor']\n",
      "WARNING:root:cannot compute similarity with no input ['toli']\n",
      "WARNING:root:cannot compute similarity with no input ['jiaye']\n",
      "WARNING:root:cannot compute similarity with no input ['leila']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute similarity with no input ['tula']\n",
      "WARNING:root:cannot compute similarity with no input ['kyra']\n",
      "WARNING:root:cannot compute similarity with no input ['zefir']\n",
      "WARNING:root:cannot compute similarity with no input ['kyra']\n",
      "WARNING:root:cannot compute similarity with no input ['menphi']\n",
      "WARNING:root:cannot compute similarity with no input ['bomi']\n",
      "WARNING:root:cannot compute similarity with no input ['camparo']\n",
      "WARNING:root:cannot compute similarity with no input ['giada']\n",
      "WARNING:root:cannot compute similarity with no input ['rhett']\n",
      "WARNING:root:cannot compute similarity with no input ['toli']\n",
      "WARNING:root:cannot compute similarity with no input ['alamo']\n",
      "WARNING:root:cannot compute similarity with no input ['zena']\n",
      "WARNING:root:cannot compute similarity with no input ['selah']\n",
      "WARNING:root:cannot compute similarity with no input ['agnes']\n",
      "WARNING:root:cannot compute similarity with no input ['leila']\n",
      "WARNING:root:cannot compute similarity with no input ['astero']\n",
      "WARNING:root:cannot compute similarity with no input ['sekoya']\n",
      "WARNING:root:cannot compute similarity with no input ['manon']\n",
      "WARNING:root:cannot compute similarity with no input ['lynne']\n",
      "WARNING:root:cannot compute similarity with no input ['narita']\n",
      "WARNING:root:cannot compute similarity with no input ['yolie']\n",
      "WARNING:root:cannot compute similarity with no input ['corinne']\n",
      "WARNING:root:cannot compute similarity with no input ['rodi']\n",
      "WARNING:root:cannot compute similarity with no input ['celes']\n",
      "WARNING:root:cannot compute similarity with no input ['cancun']\n",
      "WARNING:root:cannot compute similarity with no input ['elodia']\n",
      "WARNING:root:cannot compute similarity with no input ['dalhia']\n",
      "WARNING:root:cannot compute similarity with no input ['coba']\n",
      "WARNING:root:cannot compute similarity with no input ['camparo']\n",
      "WARNING:root:cannot compute similarity with no input ['dalhia']\n",
      "WARNING:root:cannot compute similarity with no input ['kyra']\n",
      "WARNING:root:cannot compute similarity with no input ['tallulah']\n",
      "WARNING:root:cannot compute similarity with no input ['lubo']\n",
      "WARNING:root:cannot compute similarity with no input ['anja']\n",
      "WARNING:root:cannot compute similarity with no input ['samara']\n",
      "WARNING:root:cannot compute similarity with no input ['laisa']\n",
      "WARNING:root:cannot compute similarity with no input ['astero']\n",
      "WARNING:root:cannot compute similarity with no input ['febe']\n",
      "WARNING:root:cannot compute similarity with no input ['kaden']\n",
      "WARNING:root:cannot compute similarity with no input ['zena']\n",
      "WARNING:root:cannot compute similarity with no input ['saio']\n",
      "WARNING:root:cannot compute similarity with no input ['alamo']\n",
      "WARNING:root:cannot compute similarity with no input ['asun']\n",
      "WARNING:root:cannot compute similarity with no input ['menphi']\n",
      "WARNING:root:cannot compute similarity with no input ['mandine']\n",
      "WARNING:root:cannot compute similarity with no input ['agnes']\n",
      "WARNING:root:cannot compute similarity with no input ['rodi']\n",
      "WARNING:root:cannot compute similarity with no input ['delaney']\n",
      "WARNING:root:cannot compute similarity with no input ['saio']\n",
      "WARNING:root:cannot compute similarity with no input ['tammi']\n",
      "WARNING:root:cannot compute similarity with no input ['ramona']\n",
      "WARNING:root:cannot compute similarity with no input ['sekoya']\n",
      "WARNING:root:cannot compute similarity with no input ['mandine']\n",
      "WARNING:root:cannot compute similarity with no input ['jiaye']\n",
      "WARNING:root:cannot compute similarity with no input ['menphi']\n",
      "WARNING:root:cannot compute similarity with no input ['taio']\n",
      "WARNING:root:cannot compute similarity with no input ['moscot']\n",
      "WARNING:root:cannot compute similarity with no input ['aurelia']\n",
      "WARNING:root:cannot compute similarity with no input ['asun']\n",
      "WARNING:root:cannot compute similarity with no input ['bomi']\n",
      "WARNING:root:cannot compute similarity with no input ['corinne']\n",
      "WARNING:root:cannot compute similarity with no input ['korina']\n",
      "WARNING:root:cannot compute similarity with no input ['mandine']\n",
      "WARNING:root:cannot compute similarity with no input ['zena']\n",
      "WARNING:root:cannot compute similarity with no input ['tallulah']\n",
      "WARNING:root:cannot compute similarity with no input ['cecilia']\n",
      "WARNING:root:cannot compute similarity with no input ['corsica']\n",
      "WARNING:root:cannot compute similarity with no input ['zuni']\n",
      "WARNING:root:cannot compute similarity with no input ['selah']\n"
     ]
    }
   ],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "names=['description','name','details','brand_category']\n",
    "d=pd.DataFrame()\n",
    "for i in names:\n",
    "    X_new = df_cat.apply(lambda r: w2v_tokenize_text(r[i]),axis=1).values\n",
    "    X_word_average = word_averaging_list(wv,X_new)\n",
    "    X_array=pd.DataFrame(X_word_average)\n",
    "    d=pd.concat([d,X_array],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9778761061946902 (128, 64)\n",
      "0.9771386430678466 (100, 50)\n",
      "0.9749262536873157 (50, 20)\n",
      "0.9771386430678466 (30, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(d.values, y, test_size = 0.2, random_state = 0)\n",
    "size=[(128,64),(100,50),(50,20),(30,10)]\n",
    "for j in size:\n",
    "    nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=j).fit(X_train, y_train)\n",
    "    y_pred_RF = nn.predict(X_test)\n",
    "    #print(\"Actual % of the data for tag\",p,\"is\",sum(subset_for_model['attribute_value'])/len(subset_for_model)*100)\n",
    "    print(accuracy_score(y_test,y_pred_RF),j)\n",
    "    #print(\"% for \", i, \"is\",max((1-(modeling[i].sum()/len(modeling))),(modeling[i].sum()/len(modeling))),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning Keras with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# #Generate categorical values for each tag\n",
    "# one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We chose 'epoch'=50 based upon the testing accuracy and the 'batch size' was kept small (10) because we had a small training set and this would increase the computation speed as it would need fewer epochs to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9985\n",
      "Testing Accuracy:  0.9794\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(d.values, y, test_size = 0.2, random_state = 0)\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50,verbose=False,validation_data=(X_test, y_test),batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras with our own embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the Keras tokenizer to vectorize our corpus into a list of integers and this encoding is kept in the form of dictionary. We added 1 to the vocab_size because 0 is the reserved index. We also used the pad_sequence to deal with the cases of different length of words in the text sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "columns=['brand_category','details','description','name']\n",
    "for i in ['brand_category']:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(df_cat[i])\n",
    "    kii= tokenizer.texts_to_sequences(df_cat[i])\n",
    "    CD=tokenizer.word_index\n",
    "    maxlen = 500\n",
    "    kii = pad_sequences(kii, padding='post', maxlen=maxlen)\n",
    "for i in ['details','description','name']:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(df_cat[i])\n",
    "    p = tokenizer.texts_to_sequences(df_cat[i])# Adding 1 because of reserved 0 index\n",
    "    CD={**CD,**tokenizer.word_index}\n",
    "    vocab_size=len(CD)+1\n",
    "    p = pad_sequences(p, padding='post', maxlen=maxlen)\n",
    "    kii=np.concatenate((kii,p),axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6307"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6780, 2000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kii.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 50)           315400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 315,921\n",
      "Trainable params: 315,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 50\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.2260\n",
      "Testing Accuracy:  0.2294\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(kii, y, test_size = 0.2, random_state = 0)\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10,verbose=False,validation_data=(X_test, y_test),batch_size=5)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our own integer based embeddings did not give good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras with Pre-trained Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.300d.txt',CD, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6900760938490805"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#69% of vocabulary is covered through pre trained embeddings\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 50)           315400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 8)                 88        \n",
      "=================================================================\n",
      "Total params: 315,998\n",
      "Trainable params: 598\n",
      "Non-trainable params: 315,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=False))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9987\n",
      "Testing Accuracy:  0.9838\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(kii, y, test_size = 0.2, random_state = 0)\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=20,verbose=False,validation_data=(X_test, y_test),batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We got the best results using Keras with a combination of Glove and TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  http://dsgeek.com/2018/02/19/tfidf_vectors.html\n",
    "* https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "*  https://towardsdatascience.com/build-and-compare-3-models-nlp-sentiment-prediction-67320979de61\n",
    "* https://realpython.com/python-keras-text-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
